Requirements

Functional
‚Ä¢	Store key-value pairs in memory
‚Ä¢	Fast access (low latency)
‚Ä¢	Support for expiration (TTL)
‚Ä¢	Support for cache invalidation
‚Ä¢	Distributed across multiple nodes

Non-functional
‚Ä¢	High availability
‚Ä¢	High throughput
‚Ä¢	Horizontal scalability
‚Ä¢	Consistency guarantees (eventual or strong)
‚Ä¢	Fault tolerance (data replication)

APIs

Core APIs for cache interaction:
GET /cache/{key}

Description: Retrieve the cached value for a given key.


Response: 200 OK with value, 404 Not Found if key doesn't exist.


POST /cache

Description: Add or update a key-value pair.


Body:
json
CopyEdit
{
"key": "user:123",
"value": "data",
"ttl": 3600
}


DELETE /cache/{key}

Description: Remove a cached entry by key.


GET /cache/stats

Description: Return stats like hit/miss ratio, eviction count, memory usage.


Optional:

GET /cache/bulk?keys=key1,key2,...
POST /cache/bulk
DELETE /cache/prefix/{prefix}

 Core Components

1. Clients
   ‚Ä¢	Connect via SDK/HTTP/gRPC.
   ‚Ä¢	Can use smart sharding logic.

2. Load Balancer / Proxy
   ‚Ä¢	For routing requests to nodes.
   ‚Ä¢	Can use consistent hashing or rendezvous hashing.

3. Cache Nodes
   ‚Ä¢	Each stores a subset of data in memory.
   ‚Ä¢	Handles TTL, LRU/LFU eviction locally.

4. Metadata Service / Coordinator
   ‚Ä¢	Maintains list of nodes and hash ring.
   ‚Ä¢	Handles node join/leave, replication, and rebalancing.

5. Replication Mechanism
   ‚Ä¢	Each key is replicated to N nodes (e.g. master + 1 replica).
   ‚Ä¢	Follower syncs from master periodically or in real-time.

‚∏ª

üîÑ 5. Data Flow

SET(key, value):
1.	Client sends SET to load balancer.
2.	Balancer hashes key ‚Üí determines node.
3.	Node stores value in memory + sets TTL if provided.
4.	If replication is enabled, forwards data to replica(s).

GET(key):
1.	Client sends GET.
2.	Balancer directs to primary node.
3.	Returns value if found, else miss.

DELETE(key):
1.	Deletes key from primary and replicas.

‚∏ª

üß† 6. Consistent Hashing

Why?
‚Ä¢	Avoid rehashing all keys when adding/removing nodes.

How?
‚Ä¢	Nodes placed on a virtual ring.
‚Ä¢	Keys map to nearest node clockwise.

How to Implement TTL-Based Invalidation?
There are two main strategies:

üÖ∞Ô∏è Lazy Expiration (on access)
Check expiration only when the key is accessed (get/put).

‚úÖ Pros:
Simple
No extra threads/timers
Memory-efficient (you don't check unused keys)

‚ùå Cons:
Expired keys may occupy memory until accessed


üÖ±Ô∏è Active Expiration (background cleanup)
Run a background thread to periodically remove expired keys.

‚úÖ Pros:
Frees memory proactively
Keeps cache "clean"

‚ùå Cons:
Extra thread overhead
May miss tight timing or overkill for large caches


üîÅ Hybrid Approach (Recommended)
Use lazy expiration for correctness + active cleanup for memory control:

Only remove on get() to avoid serving stale data

Background thread removes expired keys to save memory

üß† How TTL works in Distributed Setting
In a multi-node cache:

TTL is set locally in each node (based on wall-clock time)

Each node independently expires keys

Optional:
If replication exists ‚Üí replicate TTL too

If strong consistency is needed ‚Üí use a central TTL controller (rare; expensive)

üöÄ Real-World Systems
System	TTL Support	Invalidation Type
Redis	Yes	Lazy + Active
Memcached	Yes	Expiry checked on access only
Hazelcast	Yes	Configurable TTL & eviction

‚úÖ Summary
Method	Best For
Lazy expiration	Simplicity, correctness
Active cleanup	Freeing up memory proactively
Hybrid	Practical production systems

Use Case	Pick This
Unordered fast lookup table	        HashMap
Sorted access or range queries  	TreeMap
Legacy thread-safe map (avoid)  	Hashtable
Thread-safe high-perf map	    ConcurrentHashMap



